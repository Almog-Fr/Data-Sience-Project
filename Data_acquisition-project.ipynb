{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a55d5e1",
   "metadata": {},
   "source": [
    "For this project we will go over the steps required by the demands of the course: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9f9ac",
   "metadata": {},
   "source": [
    "Data acquisition - We will collect the data from the websites Just eat and Menulog. We will be collecting the data using a combination of the following tools: Selenium and BeautifulSoup. We will use Selenium to open the page and BeautifulSoup to extract the data from the website. after that, we will arrange the data in a data frame and we'll save it afterwards for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fffba",
   "metadata": {},
   "source": [
    "EDA quality and comprehension - We will use different graphs and visualization techniques to examine connection between the columns we got in the data acquisition step and try to find a connection between the information that we have and the information we wish to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2a469",
   "metadata": {},
   "source": [
    "Machine Learning experiments and insights - We will use different models of machine learning and try to find the best model between those that we have learned about in this course. We will train the model according to the train test split method and then we will apply the model on the test data. afterwards, we will asses the results and see if we can actually predict if a restaurant will be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55db20b",
   "metadata": {},
   "source": [
    "Before running tests we need to gather the data. We used Selenium webdriver combined with BeautifulSoup to open the pages and overcome the restriction we received while using an API get request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbe349",
   "metadata": {},
   "source": [
    "When using the API request, we got blocked due to the fact we weren't the site owners. This is when the Selenium came handy. We used Selenium to open the website and overcome this obstacle and then we could use BeautifulSoup in order to extract all of the data from the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e18b77",
   "metadata": {},
   "source": [
    "We took websites from the following countries: England, Ireland, New Zealand and Australia. From investigation we had on the worldwide just eat websites, these websites contained the biggest variety of information on their restaurants. We decided to take restaurants from different districts of several cities in order to get the most diverse kind of data for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4f82e",
   "metadata": {},
   "source": [
    "Another thing to keep in mind is that not all countries we mentioned above use the same currency. We needed to take that in consideration since we didn't want normal data to look like an outlier because we miss-interpreted it. For that we made a dictionary with a currency converter that has Euro as default currency. We took that converter in consideration when extracting the data as can be seen in the code bellow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d4724",
   "metadata": {},
   "source": [
    "We used a function called set_arrays_the_same_length in order to make sure we get lists that are the same length. This function just inserts NaN to the list until one list reaches the length of the second list. This will help us to create the dataframe for the testing purposes since the function that creates the data frame does not allow the lists that are supposed to be the columns to be in different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e6e5bea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.just-eat.ie/area/inns_quay-dublin#10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\almog\\AppData\\Local\\Temp\\ipykernel_23612\\4143921776.py:67: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  browser.find_element_by_xpath('//*[@id=\"skipToMain\"]/div[5]/div[3]/div/div/div/div/div/div/div[2]/button[1]').click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.just-eat.ie/area/merchants_quay-dublin#10000\n",
      "https://www.just-eat.ie/area/rotunda-dublin#10000\n",
      "https://www.just-eat.co.uk/area/e1-aldgate#10000\n",
      "https://www.just-eat.co.uk/area/ls12-armley#10000\n",
      "https://www.just-eat.co.uk/area/ec2v-london#10000\n",
      "https://www.just-eat.co.uk/area/wc1-wc1#10000\n",
      "https://www.just-eat.co.uk/area/ec1a-city_of_london#10000\n",
      "https://www.just-eat.co.uk/area/ec2-liverpoolstreet#10000\n",
      "https://www.just-eat.co.uk/area/ec3v-london_city#10000\n",
      "https://www.just-eat.co.uk/area/ec2n-london#10000\n",
      "https://www.just-eat.co.uk/area/ec2r-london#10000\n",
      "https://www.just-eat.co.uk/area/nw1-regentspark#10000\n",
      "https://www.just-eat.co.uk/area/se16-rotherhithe#10000\n",
      "https://www.just-eat.co.uk/area/se1-southwark#10000\n",
      "https://www.menulog.com.au/area/2015-alexandria\n",
      "https://www.menulog.com.au/area/2037-forest-lodge\n",
      "https://www.menulog.com.au/area/2009-pyrmont\n",
      "https://www.menulog.com.au/area/2011-rushcutters-bay\n",
      "https://www.menulog.com.au/area/3053-carlton\n",
      "https://www.menulog.com.au/area/3008-docklands\n",
      "https://www.menulog.com.au/area/3051-north-melbourne\n",
      "https://www.menulog.co.nz/area/1021-ponsonby\n",
      "https://www.menulog.co.nz/area/1011-freemans-bay\n",
      "https://www.menulog.co.nz/area/6021-hataitai\n",
      "https://www.menulog.co.nz/area/6021-mount-cook\n",
      "https://www.menulog.co.nz/area/6011-wellington-central\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "from os.path import isfile, join\n",
    "import pandas as pd \n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_arrays_the_same_length(array1,array2):\n",
    "    while len(array2) < len(array1):\n",
    "        array2.append(np.nan)\n",
    "    \n",
    "    return array2\n",
    "\n",
    "total_ranking = []\n",
    "total_times = []\n",
    "total_delivery_fee = []\n",
    "total_names = []\n",
    "total_min_order = []\n",
    "total_countries = []\n",
    "total_rankers = []\n",
    "currency_converter = {    #euro base converter\n",
    "    \"Ireland\" : 1,\n",
    "    \"New Zeland\": 0.6,\n",
    "    \"Australia\" : 0.66,\n",
    "    \"England\": 1.17\n",
    "}\n",
    "\n",
    "countries = {\"Ireland\":[\"https://www.just-eat.ie/area/inns_quay-dublin#10000\",\n",
    "                        \"https://www.just-eat.ie/area/merchants_quay-dublin#10000\",\n",
    "                        \"https://www.just-eat.ie/area/rotunda-dublin#10000\"],\n",
    "             \"England\":[\"https://www.just-eat.co.uk/area/e1-aldgate#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/ls12-armley#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/ec2v-london#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/wc1-wc1#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/ec1a-city_of_london#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/ec2-liverpoolstreet#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/ec3v-london_city#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/ec2n-london#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/ec2r-london#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/nw1-regentspark#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/se16-rotherhithe#10000\",\n",
    "                        \"https://www.just-eat.co.uk/area/se1-southwark#10000\"],\n",
    "            \"Australia\": [\"https://www.menulog.com.au/area/2015-alexandria\",\n",
    "                          \"https://www.menulog.com.au/area/2037-forest-lodge\",\n",
    "                          \"https://www.menulog.com.au/area/2009-pyrmont\",\n",
    "                          \"https://www.menulog.com.au/area/2011-rushcutters-bay\",\n",
    "                          \"https://www.menulog.com.au/area/3053-carlton\",\n",
    "                          \"https://www.menulog.com.au/area/3008-docklands\",\n",
    "                         \"https://www.menulog.com.au/area/3051-north-melbourne\"],\n",
    "            \"New Zeland\":[\n",
    "                           \"https://www.menulog.co.nz/area/1021-ponsonby\",\n",
    "                           \"https://www.menulog.co.nz/area/1011-freemans-bay\",\n",
    "                           \"https://www.menulog.co.nz/area/6021-hataitai\",\n",
    "                           \"https://www.menulog.co.nz/area/6021-mount-cook\",\n",
    "                           \"https://www.menulog.co.nz/area/6011-wellington-central\"]\n",
    "}\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "for country in countries:\n",
    "    for url in countries[country]:\n",
    "        print(url)\n",
    "        browser.get(url)\n",
    "        try:\n",
    "            browser.find_element_by_xpath('//*[@id=\"skipToMain\"]/div[5]/div[3]/div/div/div/div/div/div/div[2]/button[1]').click()\n",
    "        except:\n",
    "            continue\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\") \n",
    "        #names\n",
    "        divs_names = soup.find_all(\"h3\", attrs = {\"class\":\"RestaurantCard_c-restaurantCard-name_1Zwfd\"}) \n",
    "        names = [name.get_text() for name in divs_names]\n",
    "        # print(\"names\", names)\n",
    "        total_names.extend(names) \n",
    "        #ranking \n",
    "        divs_ranking = soup.find_all(\"data\", attrs = {\"class\":\"RestaurantRating_c-restaurantCard-rating-mean_2nucs\"})\n",
    "        ranking = [rank.get_text() for rank in divs_ranking]\n",
    "        total_ranking.extend(ranking)\n",
    "        # print(\"ranking\", ranking)\n",
    "        divs_rankers = soup.find_all(\"data\", attrs = {\"class\":\"RestaurantRating_c-restaurantCard-rating-count_1HT6D\"})\n",
    "        rankers = [ranker.get_text() for ranker in divs_rankers]\n",
    "        total_rankers.extend(rankers)\n",
    "        #Country \n",
    "        for i in range(len(names)): \n",
    "            total_countries.append(country)\n",
    "        #time, min order, delivery fee\n",
    "        divs = soup.find_all(\"span\", attrs = {\"class\":\"IconText_c-restaurantCard-iconText-content_2wOUu\"}) \n",
    "        for anomaly in divs:\n",
    "            if \"mins\" in anomaly.get_text():\n",
    "                total_times.append(int(anomaly.get_text()[5:8]))\n",
    "            elif \"Delivery\" in anomaly.get_text():\n",
    "                if \"FREE\" in anomaly.get_text():\n",
    "                    total_delivery_fee.append(0)\n",
    "                else:                                         \n",
    "                    if len(anomaly.get_text()) == 14:\n",
    "                        if \"from\" in anomaly.get_text():\n",
    "                            total_delivery_fee.append(float(anomaly.get_text()[10:]) * currency_converter[country])\n",
    "                        else:\n",
    "                            total_delivery_fee.append(float(anomaly.get_text()[11:13]) * currency_converter[country])\n",
    "                    elif len(anomaly.get_text()) == 19:\n",
    "                        total_delivery_fee.append(float(anomaly.get_text()[15:])* currency_converter[country])\n",
    "                    elif len(anomaly.get_text()) == 16:\n",
    "                        total_delivery_fee.append(float(anomaly.get_text()[11:15])* currency_converter[country])\n",
    "                    elif len(anomaly.get_text()) == 21:      \n",
    "                        total_delivery_fee.append(float(anomaly.get_text()[16:20])* currency_converter[country])\n",
    "                    else:\n",
    "                        total_delivery_fee.append('')  \n",
    "            elif \"min.\" in anomaly.get_text() or \"Min.\" in anomaly.get_text(): \n",
    "                if \"No\" in anomaly.get_text():\n",
    "                    total_min_order.append(0)\n",
    "                elif len(anomaly.get_text()) > 13:\n",
    "                        total_min_order.append(float(anomaly.get_text()[13:])* currency_converter[country])\n",
    "                else:\n",
    "                    total_min_order.append('')\n",
    "    total_ranking = set_arrays_the_same_length(total_names,total_ranking)\n",
    "    total_times = set_arrays_the_same_length(total_names,total_times)\n",
    "    total_delivery_fee = set_arrays_the_same_length(total_names,total_delivery_fee)\n",
    "    total_min_order = set_arrays_the_same_length(total_names,total_min_order)\n",
    "    total_countries = set_arrays_the_same_length(total_names,total_countries)\n",
    "    total_rankers = set_arrays_the_same_length(total_names,total_rankers)\n",
    "df = pd.DataFrame({\"names\":total_names,\"countries\":total_countries,\"min_order\":total_min_order,\"amount_of_rankers\":total_rankers,\"delivery_fee\":total_delivery_fee,\"delivery_time_MAX\":total_times,\"ranking\":total_ranking})\n",
    "df.to_csv(\"C:\\\\Users\\\\almog\\\\Desktop\\\\Data-Sience-Project-main1\\\\DataFrame.csv\")\n",
    "print(\"Done!\\n\")     \n",
    "browser.quit()                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fee70f",
   "metadata": {},
   "source": [
    "We finished the first step of the project. We gathered the data and made a data frame out of it. The next step from here is to clean the data a little bit and prepare it for further analysis which will be made in a different notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8460a",
   "metadata": {},
   "source": [
    "During the gathering of the data we had to use a lot of conditions. we needed to get the values separated since all of them used the same class in the site. after gathering all of the values we balanced the lists with our function set_arrays_the_same_lenths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1484c8ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
